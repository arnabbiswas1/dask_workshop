{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 10004\n",
    "matplotlib.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('/opt/vssexclude/personal/kaggle/volcano/src/'))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Data\n",
    "\n",
    "Data used in this notebook is from the Kaggle Competition \"INGV - Volcanic Eruption Prediction\"(https://www.kaggle.com/c/predict-volcanic-eruptions-ingv-oe).\n",
    "\n",
    "We will explore a bunch of files under `train` and `test` directories. Each file contains ten minutes of logs from ten different sensors arrayed around a volcano. There are 4432 data files under the train directory and 4521 files under test directory. Each of these files consists of 60K lines. On the disk, size of the files under train and test directory is 30G (15G + 15G).\n",
    "\n",
    "## Challenges with Large Data\n",
    "\n",
    "As a Data Scientist, we encounter two major challenges when dealing with such a large volume of data:\n",
    "\n",
    "1. Limited Processing Power: Because of Python's Global Interpreter Lock (GIL), libraries like Pandas or Numpy can use only one processor at any point of time, even when multiple processors available. \n",
    "\n",
    "2. Limited Memory: For a workstation, RAM is often limited to 16 or 32 GB. So, it's kind of impossible to load all the data files together. Even the disk space will be limited to around 2 TB.\n",
    "\n",
    "## Why Dask?\n",
    "Dask is a framework designed to overcome these limitations:\n",
    "\n",
    "1. Parallelization using Multiple Cores (avilable in a Single Computer or distributed across multiple computers)\n",
    "2. Out of Core Computing: If size of the data is larger than the main memory (RAM), dask doesn't load all the data in-memory at a time. It streams the data from the disk as and when needed. If Data doesn't fit into the disk of a single computer, it can be ditributed across multiple computers.\n",
    "\n",
    "Dask can scale on thousand-machine clusters to handle hundreds of terabytes of data. At the same time, it works efficiently on a single machine as well, enabling analysis of moderately large datasets (100GB+) on relatively low power laptops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many Processors do I have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equal to double the number of CPU Cores since in most of the Computers hyperthreading is enabled.\n",
    "\n",
    "**Hyperthreading** tells the operating system that it has two cores for every physical core. In my Window's laptop, I have 6 physical cores, but 12 logical processors. But, these 12 logical processors will not give 12x improvement compared to single physical core. Hyperthreading generally gives around 1.25x to 1.3x improvement if two cores give 2x improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/dask_architechture_diagram.png\" width=\"600\" height=\"200\" style=\"border-style: solid;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a Client?\n",
    "\n",
    "The Client connects users to a Dask cluster. After a Dask cluster is setup, we initialize a Client by pointing it to the address of a Scheduler:\n",
    "\n",
    "```python\n",
    "from distributed import Client\n",
    "client = Client(\"1.2.3.4:8786\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Dask Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client(n_workers=10, memory_limit='2.5GB')\n",
    "client = Client(n_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating a Client without specifying the scheduler (cluster) address. In this case, the Client creates a `LocalCluster` in the background and connects to that. Any computation will automatically use this `LocalCluster`.\n",
    "\n",
    "The above code is effectively same as the following:\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=10)\n",
    "client = Client(cluster)\n",
    "```\n",
    "\n",
    "A client can be closed using:\n",
    "\n",
    "```python\n",
    "client.close()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the directory where all the CSV files are stored\n",
    "DATA_DIR = \"/opt/vssexclude/personal/kaggle/volcano/data/raw/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many files are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4431\n"
     ]
    }
   ],
   "source": [
    "! ls {DATA_DIR} | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a quick look of the content of an individual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_1,sensor_2,sensor_3,sensor_4,sensor_5,sensor_6,sensor_7,sensor_8,sensor_9,sensor_10\n",
      "36.0,1050.0,155.0,643.0,-39.0,843.0,-21.0,96.0,-62.0,-1243.0\n",
      "8.0,856.0,218.0,705.0,10.0,763.0,-117.0,90.0,-165.0,-1348.0\n"
     ]
    }
   ],
   "source": [
    "! head -3 {DATA_DIR}/2019378960.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the size of one of an individual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 arnabb arnabb 3.6M Oct  8 23:59 /opt/vssexclude/personal/kaggle/volcano/data/raw/train/2019378960.csv\n"
     ]
    }
   ],
   "source": [
    "! ls -lh {DATA_DIR}/2019378960.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the size of all the files together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15G\t/opt/vssexclude/personal/kaggle/volcano/data/raw/train\n"
     ]
    }
   ],
   "source": [
    "! du -h {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datatypes for different sensor data\n",
    "data_types = {\"sensor_1\" : np.float32, \n",
    "                 \"sensor_2\" : np.float32, \n",
    "                 \"sensor_3\" : np.float32,\n",
    "                 \"sensor_4\" : np.float32,\n",
    "                 \"sensor_5\" : np.float32,\n",
    "                 \"sensor_6\" : np.float32,\n",
    "                 \"sensor_7\" : np.float32,\n",
    "                 \"sensor_8\" : np.float32,\n",
    "                 \"sensor_9\" : np.float32,\n",
    "                 \"sensor_10\" : np.float32}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all the files using Panda\n",
    "\n",
    "- Need to do it sequentially\n",
    "- Memory occupied by the objects must be less than the Main Memory (RAM)\n",
    "\n",
    "```python\n",
    "%%time\n",
    "for name in os.listdir(DATA_DIR):\n",
    "    df = pd.read_csv(f\"{DATA_DIR}/{name}\", dtype=data_types)\n",
    "```\n",
    "\n",
    "```\n",
    "CPU times: user 2min 30s, sys: 1min 5s, total: 3min 35s\n",
    "Wall time: 3min 42s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all the files using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 666 ms, sys: 40.6 ms, total: 707 ms\n",
      "Wall time: 761 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-232.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-120.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  \\\n",
       "0     260.0      64.0    -232.0     -36.0      -2.0     -35.0     103.0   \n",
       "1     233.0     175.0     146.0     160.0      -4.0      29.0    -120.0   \n",
       "\n",
       "   sensor_8  sensor_9  sensor_10  \n",
       "0     389.0      67.0       41.0  \n",
       "1     498.0      59.0       63.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# blocksize is set to value None. \n",
    "# A single block is used for each file\n",
    "dd_seg = dd.read_csv(urlpath=f\"{DATA_DIR}/*.csv\", blocksize=None, dtype=data_types)\n",
    "\n",
    "dd_seg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in the Dask DataFrame: 4431\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of partitions in the Dask DataFrame: {dd_seg.npartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
